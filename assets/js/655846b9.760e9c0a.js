"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4418],{2574:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var t=a(4848),s=a(8453);const r={},o="Deploying a Monitoring stack with HA",i={id:"launchpad/tutorials/monitoring-stack-with-HA",title:"Deploying a Monitoring stack with HA",description:"Prerequisites",source:"@site/docs/launchpad/tutorials/monitoring-stack-with-HA.md",sourceDirName:"launchpad/tutorials",slug:"/launchpad/tutorials/monitoring-stack-with-HA",permalink:"/launchpad/tutorials/monitoring-stack-with-HA",draft:!1,unlisted:!1,editUrl:"https://github.com/graphops/docs/edit/main/docs/launchpad/tutorials/monitoring-stack-with-HA.md",tags:[],version:"current",frontMatter:{},sidebar:"launchpadSidebar",previous:{title:"Celo Archive Mainnet Node Guide",permalink:"/launchpad/tutorials/celo-archive-kubernetes-guide"},next:{title:"Overview of High Availability in PostgreSQL",permalink:"/launchpad/tutorials/postgresql_ha"}},l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Configuring Loki for HA",id:"configuring-loki-for-ha",level:2},{value:"Setting up a Prometheus Stack with HA",id:"setting-up-a-prometheus-stack-with-ha",level:2},{value:"Thanos",id:"thanos",level:3},{value:"Prometheus Stack",id:"prometheus-stack",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"deploying-a-monitoring-stack-with-ha",children:"Deploying a Monitoring stack with HA"})}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A fully functional working Kubernetes cluster"}),"\n",(0,t.jsx)(n.li,{children:"Two object storage buckets: one for Logs data, used by Loki, and one for Metrics data, used by Thanos"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"configuring-loki-for-ha",children:"Configuring Loki for HA"}),"\n",(0,t.jsxs)(n.p,{children:["Launchpad uses the ",(0,t.jsx)(n.a,{href:"https://github.com/grafana/helm-charts/tree/main/charts/loki-distributed",children:"loki-distributed"})," release for setting up Loki, which can be configured according to its values interface (as seen ",(0,t.jsx)(n.a,{href:"https://github.com/grafana/helm-charts/blob/main/charts/loki-distributed/values.yaml",children:"here"}),")"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"Note"}),": The example setups we'll show will be based on an architecture that makes use of the following components: querier, distributor, ingester, queryFrontend, gateway, compactor, ruler, indexGateway. Different architectures are possible so adjust to your needs as necessary."]}),"\n",(0,t.jsx)(n.p,{children:"For an HA setup, deploying several components with multiple replicas each, loki-distributed values can be set like in the following example snippet:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"querier:\n  replicas: 2\n  maxUnavailable: 1\ndistributor:\n  replicas: 3\n  maxUnavailable: 2\ningester:\n  replicas: 3\n  maxUnavailable: 2\nqueryFrontend:\n  replicas: 2\n  maxUnavailable: 1\ngateway:\n  replicas: 2\n  maxUnavailable: 1\ncompactor:\n  kind: Deployment\n  replicas: 1\n  enabled: true\nruler:\n  enabled: true\n  replicas: 2\n  maxUnavailable: 1\nindexGateway:\n  enabled: true\n  replicas: 2\n  maxUnavailable: 1\nloki:\n  structuredConfig:\n    ruler:\n      ring:\n        kvstore:\n          store: memberlist\n    ingester:\n      lifecycler:\n        ring:\n          replication_factor: 2\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"Note"}),": If you use a compactor, only one will run at a time and it's not critical so you don't really need more than one instance of it."]}),"\n",(0,t.jsxs)(n.p,{children:["Besides increasing the number of replicas, ingester ",(0,t.jsx)(n.code,{children:"replication_factor"})," is of particular relevance as the Distributor will distribute the write load to multiple ingesters and will require a quorum of them to have acknowledged the write (replication_factor / 2 + 1). For lowering the chances of loosing logs, a replication_factor of at least two should be used (Loki default is 3)."]}),"\n",(0,t.jsx)(n.p,{children:"Loki's storage fundamentally requires object storage, regardless of whether HA is used or if there's more than one replica for any component, as multiple components need to share this storage."}),"\n",(0,t.jsx)(n.p,{children:"Object storage can be setup as shown in the following snippet:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'loki:\n  structuredConfig:\n    storage_config:\n      tsdb_shipper:\n        active_index_directory: /var/loki/data/tsdb-index\n        cache_location: /var/loki/data/tsdb-cache\n        index_gateway_client:\n          # only applicable if using microservices where index-gateways are independently deployed.\n          # This example is using kubernetes-style naming.\n          server_address: dns:///loki-loki-distributed-index-gateway.monitoring.svc.cluster.local:9095\n        shared_store: s3\n      aws:\n        bucketnames: <<bucket>>\n        endpoint: <<endpoint>>\n        region: <<region>>\n        access_key_id: "${S3_ACCESS_KEY_ID}"\n        secret_access_key: "${S3_SECRET_ACCESS_KEY}"\n        insecure: false\n        sse_encryption: false\n        s3forcepathstyle: true\n    schema_config:\n      configs:\n        # New TSDB schema below\n        - from: "2024-01-01"\n          index:\n            period: 24h\n            prefix: index_\n          object_store: s3\n          schema: v12\n          store: tsdb\n    query_scheduler:\n      # the TSDB index dispatches many more, but each individually smaller, requests.\n      # We increase the pending request queue sizes to compensate.\n      max_outstanding_requests_per_tenant: 32768\n    querier:\n      max_concurrent: 16\n    compactor:\n      shared_store: s3\nquerier:\n  extraArgs:\n    - -config.expand-env=true\n  extraEnv:\n    - name: S3_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_ACCESS_KEY_ID\n    - name: S3_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_SECRET_ACCESS_KEY\ndistributor:\n  extraArgs:\n    - -config.expand-env=true\n  extraEnv:\n    - name: S3_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_ACCESS_KEY_ID\n    - name: S3_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_SECRET_ACCESS_KEY\ningester:\n  extraArgs:\n    - -config.expand-env=true\n  extraEnv:\n    - name: S3_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_ACCESS_KEY_ID\n    - name: S3_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_SECRET_ACCESS_KEY\ncompactor:\n  extraArgs:\n    - -config.expand-env=true\n  extraEnv:\n    - name: S3_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_ACCESS_KEY_ID\n    - name: S3_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_SECRET_ACCESS_KEY\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Here we are setting up object storage in the ",(0,t.jsx)(n.code,{children:"structuredConfig"})," section, but to keep the credentials secret, we are adding env vars from secrets to several components and an extra command line argument ",(0,t.jsx)(n.code,{children:"-config.expand-env=true"}),", the purpose of which is being able to use ENV vars in the ",(0,t.jsx)(n.code,{children:"structuredConfig"})," section. With that argument, the components will replace the values such as ",(0,t.jsx)(n.code,{children:"${S3_ACCESS_KEY_ID}"})," by the corresponding ENV var value upon processing the config."]}),"\n",(0,t.jsx)(n.p,{children:"Besides setting up object storage, we're also configuring TSDB index schema in substitution of the default boltdb-shipper, which is a more recent and more efficient alternative to it. Doing so is not mandatory but recommended."}),"\n",(0,t.jsxs)(n.p,{children:["Putting it all together and adding a few more standard options such as persistence (PVC) to some components, and enabling ServiceMonitor and Prometheus Rules, a Launchpad Monitoring namespace ",(0,t.jsx)(n.code,{children:"helmfile.yaml"})," Loki config could look like:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'helmfiles:\n  - path: git::https://github.com/graphops/launchpad-namespaces.git@monitoring/helmfile.yaml?ref=monitoring-stable/latest\n    selectorsInherited: true\n    values:\n    - features: [ metrics, logs]\n      loki:\n        values:\n          loki:\n            structuredConfig:\n              ingester:\n                # Disable chunk transfer which is not possible with statefulsets\n                # and unnecessary for boltdb-shipper\n                max_transfer_retries: 0\n                chunk_idle_period: 1h\n                chunk_target_size: 1536000\n                max_chunk_age: 1h\n              storage_config:\n                tsdb_shipper:\n                  active_index_directory: /var/loki/data/tsdb-index\n                  cache_location: /var/loki/data/tsdb-cache\n                  index_gateway_client:\n                    # only applicable if using microservices where index-gateways are independently deployed.\n                    # This example is using kubernetes-style naming.\n                    server_address: dns:///loki-loki-distributed-index-gateway.monitoring.svc.cluster.local:9095\n                  shared_store: s3\n                aws:\n                  bucketnames: <<bucket>>\n                  endpoint: <<endpoint>>\n                  region: <<region>>\n                  access_key_id: "${S3_ACCESS_KEY_ID}"\n                  secret_access_key: "${S3_SECRET_ACCESS_KEY}"\n                  insecure: false\n                  sse_encryption: false\n                  s3forcepathstyle: true\n              schema_config:\n                configs:\n                  # New TSDB schema below\n                  - from: "2024-01-01"\n                    index:\n                      period: 24h\n                      prefix: index_\n                    object_store: s3\n                    schema: v12\n                    store: tsdb\n              query_scheduler:\n                # the TSDB index dispatches many more, but each individually smaller, requests.\n                # We increase the pending request queue sizes to compensate.\n                max_outstanding_requests_per_tenant: 32768\n              querier:\n                # Each `querier` component process runs a number of parallel workers to process queries simultaneously.\n                # You may want to adjust this up or down depending on your resource usage\n                # (more available cpu and memory can tolerate higher values and vice versa),\n                # but we find the most success running at around `16` with tsdb\n                max_concurrent: 16\n              compactor:\n                shared_store: s3\n              ruler:\n                ring:\n                  kvstore:\n                    store: memberlist\n                rule_path: /tmp/loki/scratch\n                alertmanager_url: http://kube-prometheus-stack-alertmanager:9093\n                external_url: <<your alertmanager external URL>>\n\n          querier:\n            replicas: 2\n            maxUnavailable: 1\n            extraArgs:\n              - -config.expand-env=true\n            extraEnv:\n              - name: S3_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_ACCESS_KEY_ID\n              - name: S3_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_SECRET_ACCESS_KEY\n          distributor:\n            replicas: 3\n            maxUnavailable: 2\n            extraArgs:\n              - -config.expand-env=true\n            extraEnv:\n              - name: S3_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_ACCESS_KEY_ID\n              - name: S3_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_SECRET_ACCESS_KEY\n          ingester:\n            replicas: 3\n            maxUnavailable: 2\n            persistence:\n              enabled: true\n              inMemory: false\n              claims:\n                - name: data\n                  size: 10Gi\n            extraArgs:\n              - -config.expand-env=true\n            extraEnv:\n              - name: S3_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_ACCESS_KEY_ID\n              - name: S3_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_SECRET_ACCESS_KEY\n          queryFrontend:\n            replicas: 2\n            maxUnavailable: 1\n          gateway:\n            replicas: 2\n            maxUnavailable: 1\n          compactor:\n            kind: Deployment\n            replicas: 1\n            enabled: true\n            extraArgs:\n              - -config.expand-env=true\n            extraEnv:\n              - name: S3_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_ACCESS_KEY_ID\n              - name: S3_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_SECRET_ACCESS_KEY\n          ruler:\n            enabled: true\n            replicas: 2\n            maxUnavailable: 1\n          indexGateway:\n            enabled: true\n            replicas: 2\n            maxUnavailable: 1\n          serviceMonitor:\n            enabled: true\n          prometheusRule:\n            enabled: true\n            namespace: monitoring\n'})}),"\n",(0,t.jsx)(n.p,{children:"We've also added setting up Ruler with Alertmanager's endpoint (which can be deployed by the Monitoring Namespace as well, as will be seen in the Metrics section)."}),"\n",(0,t.jsx)(n.h2,{id:"setting-up-a-prometheus-stack-with-ha",children:"Setting up a Prometheus Stack with HA"}),"\n",(0,t.jsx)(n.h3,{id:"thanos",children:"Thanos"}),"\n",(0,t.jsxs)(n.p,{children:["For an HA Prometheus Stack we'll need ",(0,t.jsx)(n.a,{href:"https://github.com/thanos-io/thanos",children:"Thanos"})," which is not yet part of the Monitoring Namespace, so we'll start by going over how to deploy it with Launchpad."]}),"\n",(0,t.jsxs)(n.p,{children:["Thanos requires object storage so a bucket (and credentials) will be needed. To deploy Thanos we're going to use bitnami's ",(0,t.jsx)(n.a,{href:"https://github.com/bitnami/charts/tree/main/bitnami/thanos",children:"thanos chart"}),", and we'll deploy that with Launchpad as in the following example helmfile:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'repositories:\n- name: bitnami\n  url: https://charts.bitnami.com/bitnami\n          \nreleases:\n  - name: thanos\n    namespace: monitoring\n    createNamespace: true\n    chart: bitnami/thanos\n    version: ~12.20\n    missingFileHandler: Warn\n    values:\n    - existingObjstoreSecret: <<thanos-objstore-secret>>\n      query:\n        replicaCount: 2\n        dnsDiscovery:\n          sidecarsService: "prometheus-operated"\n          sidecarsNamespace: "monitoring"\n        replicaLabel:\n          - prometheus_replica\n      queryFrontend:\n        enabled: true\n        replicaCount: 2\n      compactor:\n        enabled: true\n        persistence:\n          enabled: true\n        retentionResolutionRaw: 30d\n        retentionResolution5m: 30d\n        retentionResolution1h: 10y\n      storegateway:\n        enabled: true\n        replicaCount: 2\n        persistence:\n          enabled: true\n      metrics:\n        enabled: true\n        serviceMonitor:\n          enabled: true\n      prometheusRule:\n        enabled: true\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"Warning"}),": Never try to run more than one instance of compactor. If your object storage does not support locking, it will lead to error states."]}),"\n",(0,t.jsxs)(n.p,{children:["Where we added the bitnami repository, and a release to deploy the Thanos chart from that repository. From the values used in this example, notice the ",(0,t.jsx)(n.code,{children:"query.dnsDiscovery"})," and ",(0,t.jsx)(n.code,{children:"query.replicaLabel"})," keys, as those values need to match the ones used in Thanos Prometheus sidecar, deployed in the ",(0,t.jsx)(n.code,{children:"kube-prometheus-stack"})," release with the Monitoring Namespace."]}),"\n",(0,t.jsxs)(n.p,{children:["There is one extra thing needed for Thanos, a secret with the bucket credentials as referred previously with ",(0,t.jsx)(n.code,{children:"<<thanos-objstore-secret>>"}),". That secret need to have a key called ",(0,t.jsx)(n.code,{children:"objstore.yml"}),", and its value content should be yaml and have keys like these:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"type: S3\nconfig: \n  endpoint: <<endpoint>>\n  bucket: <<bucket name>>\n  bucket_lookup_type: path \n  insecure: false\n  access_key: <<access_key>>\n  secret_key: <<secret_key>>\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"bucket_lookup_type"})," can be ",(0,t.jsx)(n.code,{children:"auto"}),", ",(0,t.jsx)(n.code,{children:"path"})," or ",(0,t.jsx)(n.code,{children:"virtual_host"}),", and you would want to use ",(0,t.jsx)(n.code,{children:"path"})," for Ceph Object Storage. You can check ",(0,t.jsx)(n.a,{href:"https://thanos.io/tip/thanos/storage.md/#s3",children:"here"})," all the available options."]}),"\n",(0,t.jsx)(n.p,{children:"Adding a secret like that with Launchpad and Sealed Secrets will add a release like so:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"releases:\n  - name: thanos-objstore-secret\n    namespace: monitoring\n    chart: graphops/resource-injector\n    values:\n      - resources:\n          thanos-objstore-secret:\n            apiVersion: bitnami.com/v1alpha1\n            kind: SealedSecret\n            metadata:\n              name: thanos-objstore-secret\n              namespace: monitoring\n            spec:\n              encryptedData:\n                objstore.yml: <<SealedSecrets Encrypted Data>>\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The last remaining required Thanos component, the prometheus sidecar, will be deployed with ",(0,t.jsx)(n.code,{children:"kube-prometheus-stack"})," so keep reading."]}),"\n",(0,t.jsx)(n.h3,{id:"prometheus-stack",children:"Prometheus Stack"}),"\n",(0,t.jsx)(n.p,{children:"There are three components we want to focus on, Prometheus, Grafana and Alertmanager.\nWe'll start with adjusting Alertmanager's config for HA which is the simplest:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"alertmanager:\n  alertmanagerSpec:\n    replicas: 3\n"})}),"\n",(0,t.jsx)(n.p,{children:"This will change our setup to a 3 replica Alertmanager and that's all that's required."}),"\n",(0,t.jsx)(n.p,{children:"For grafana we have the added requirement of changing from the default embedded sqlite database to a shared database like Postgres."}),"\n",(0,t.jsx)(n.p,{children:"So let's start by adding a release to create a Postgres database with the Postgres-Operator (from Launchpad Postgres-Operator Namespace). As an example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'releases:\n  - name: grafana-database\n    namespace: monitoring\n    createNamespace: true\n    chart: graphops/resource-injector\n    version: 0.2.0\n    missingFileHandler: Warn\n    values:\n      - resources:\n          grafana-database:\n            apiVersion: "acid.zalan.do/v1"\n            kind: postgresql\n            metadata:\n              name: grafana-database\n              teamId: "pg"\n              numberOfInstances: 2\n              users:\n                grafana:\n                  - superuser\n                  - createdb\n              enableMasterLoadBalancer: false\n              enableReplicaLoadBalancer: false\n              enableConnectionPooler: false\n              enableReplicaConnectionPooler: false\n              databases:\n                grafana: grafana\n              postgresql:\n                version: "15"\n                parameters: {}\n              volume:\n                size: 1Gi\n                storageClass: <<your_storage_class>>\n              resources:\n                requests:\n                  cpu: 250m\n                  memory: 1Gi\n                limits:\n                  cpu: 1000m\n                  memory: 4Gi\n              patroni:\n                initdb:\n                  encoding: "UTF8"\n                  locale: "C"\n                pg_hba:\n                  - local   all             all                                   trust\n                  - hostssl all             +zalandos    127.0.0.1/32       pam\n                  - host    all             all                127.0.0.1/32       md5\n                  - hostssl all             +zalandos    ::1/128            pam\n                  - host    all             all                ::1/128            md5\n                  - local   replication     standby                    trust\n                  - hostssl replication     standby all                md5\n                  - hostnossl all           all                all                md5\n                  - hostssl all             +zalandos    all                pam\n                  - hostssl all             all                all                md5\n              podAnnotations:\n                coa.zalan.do/auto-create-database: "true"\n'})}),"\n",(0,t.jsx)(n.p,{children:"Having that database, adjusting grafana values setup can be achieved like so:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'grafana:\n  replicas: 2\n  envValueFrom:\n    DATABASE_PASSWORD:\n      secretKeyRef:\n        name: grafana.grafana-database.credentials.postgresql.acid.zalan.do\n        key: password\n  sidecar:\n    datasources:\n      url: http://thanos-query-frontend:9090\n      createPrometheusReplicasDatasources: false\n  grafana.ini:\n    database:\n      type: postgres\n      host: grafana-database.monitoring.svc:5432\n      name: grafana\n      user: grafana\n      password: "$__env{DATABASE_PASSWORD}"\n'})}),"\n",(0,t.jsx)(n.p,{children:"Finally, we need to adjust Prometheus to increase replicas and use Thanos sidecar."}),"\n",(0,t.jsx)(n.p,{children:"A workable set of values for accomplishing that looks like:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"prometheus:\n  prometheusSpec:\n    replicas: 2\n    shards: 1\n    thanos:\n      objectStorageConfig:\n        existingSecret:\n          name: <<thanos-objstore-secret>>\n          key: objstore.yml\n    replicaExternalLabelName: prometheus_replica\n  thanosService:\n    enabled: true\n  thanosServiceMonitor:\n    enabled: true\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Notice the sidecar will be configured to use the same secret provisioned before for Thanos, and the ",(0,t.jsx)(n.code,{children:"replicaExternalLabelName"})," matches the value used before as well."]}),"\n",(0,t.jsxs)(n.p,{children:["Taking all of this together, here's an example of an helmfile that deploys Thanos and ",(0,t.jsx)(n.code,{children:"kube-prometheus-stack"})," setting the most important values for HA:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'repositories:\n- name: bitnami\n  url: https://charts.bitnami.com/bitnami\n\nhelmfiles:\n  - path: git::https://github.com/graphops/launchpad-namespaces.git@monitoring/helmfile.yaml?ref=monitoring-stable/latest\n    selectorsInherited: true\n    values:\n    - helmDefaults:\n        <<: *helmDefaults\n      features: [ metrics, logs]\n      kube-prometheus-stack:\n        values:\n          kube-prometheus-stack:\n            alertmanager:\n              alertmanagerSpec:\n                replicas: 3\n            grafana:\n              replicas: 2\n              envValueFrom:\n                DATABASE_PASSWORD:\n                  secretKeyRef:\n                    name: grafana.grafana-database.credentials.postgresql.acid.zalan.do\n                    key: password\n              sidecar:\n                datasources:\n                  url: http://thanos-query-frontend:9090\n                  createPrometheusReplicasDatasources: false\n              grafana.ini:\n                database:\n                  type: postgres\n                  host: grafana-database.monitoring.svc:5432\n                  name: grafana\n                  user: grafana\n                  password: "$__env{DATABASE_PASSWORD}"\n            prometheus:\n              prometheusSpec:\n                replicas: 2\n                shards: 1\n                thanos:\n                  objectStorageConfig:\n                    existingSecret:\n                      name: <<thanos-objstore-secret>>\n                      key: objstore.yml\n                replicaExternalLabelName: prometheus_replica\n              thanosService:\n                enabled: true\n              thanosServiceMonitor:\n                enabled: true\n\nreleases:\n  - name: thanos-objstore-secret\n    namespace: monitoring\n    chart: graphops/resource-injector\n    values:\n      - resources:\n          thanos-objstore-secret:\n            apiVersion: bitnami.com/v1alpha1\n            kind: SealedSecret\n            metadata:\n              name: thanos-objstore-secret\n              namespace: monitoring\n            spec:\n              encryptedData:\n                objstore.yml: <<SealedSecrets Encrypted Data>>\n\n  - name: thanos\n    namespace: monitoring\n    createNamespace: true\n    chart: bitnami/thanos\n    version: ~12.20\n    missingFileHandler: Warn\n    values:\n    - existingObjstoreSecret: <<thanos-objstore-secret>>\n      query:\n        replicaCount: 2\n        dnsDiscovery:\n          sidecarsService: "prometheus-operated"\n          sidecarsNamespace: "monitoring"\n        replicaLabel:\n          - prometheus_replica\n      queryFrontend:\n        enabled: true\n        replicaCount: 2\n      compactor:\n        enabled: true\n        persistence:\n          enabled: true\n        retentionResolutionRaw: 30d\n        retentionResolution5m: 30d\n        retentionResolution1h: 10y\n      storegateway:\n        enabled: true\n        replicaCount: 2\n        persistence:\n          enabled: true\n      metrics:\n        enabled: true\n        serviceMonitor:\n          enabled: true\n      prometheusRule:\n        enabled: true\n\n  - name: grafana-database\n    namespace: monitoring\n    createNamespace: true\n    chart: graphops/resource-injector\n    version: 0.2.0\n    missingFileHandler: Warn\n    values:\n      - resources:\n          grafana-database:\n            apiVersion: "acid.zalan.do/v1"\n            kind: postgresql\n            metadata:\n              name: grafana-database\n              teamId: "pg"\n              numberOfInstances: 2\n              users:\n                grafana:\n                  - superuser\n                  - createdb\n              enableMasterLoadBalancer: false\n              enableReplicaLoadBalancer: false\n              enableConnectionPooler: false\n              enableReplicaConnectionPooler: false\n              databases:\n                grafana: grafana\n              postgresql:\n                version: "15"\n                parameters: {}\n              volume:\n                size: 1Gi\n                storageClass: <<your_storage_class>>\n              resources:\n                requests:\n                  cpu: 250m\n                  memory: 1Gi\n                limits:\n                  cpu: 1000m\n                  memory: 4Gi\n              patroni:\n                initdb:\n                  encoding: "UTF8"\n                  locale: "C"\n                pg_hba:\n                  - local   all             all                                   trust\n                  - hostssl all             +zalandos    127.0.0.1/32       pam\n                  - host    all             all                127.0.0.1/32       md5\n                  - hostssl all             +zalandos    ::1/128            pam\n                  - host    all             all                ::1/128            md5\n                  - local   replication     standby                    trust\n                  - hostssl replication     standby all                md5\n                  - hostnossl all           all                all                md5\n                  - hostssl all             +zalandos    all                pam\n                  - hostssl all             all                all                md5\n              podAnnotations:\n                coa.zalan.do/auto-create-database: "true"\n\n'})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>i});var t=a(6540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);