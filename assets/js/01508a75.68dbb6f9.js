"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8386],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>h});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),u=p(a),m=r,h=u["".concat(l,".").concat(m)]||u[m]||d[m]||o;return a?n.createElement(h,s(s({ref:t},c),{},{components:a})):n.createElement(h,s({ref:t},c))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,s=new Array(o);s[0]=m;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[u]="string"==typeof e?e:r,s[1]=i;for(var p=2;p<o;p++)s[p]=a[p];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},3245:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const o={},s="Overview of High Availability in PostgreSQL",i={unversionedId:"launchpad/tutorials/postgresql_ha",id:"launchpad/tutorials/postgresql_ha",title:"Overview of High Availability in PostgreSQL",description:"One of the prerequisites of running an indexer stack is currently using PostgreSQL as a database for storing indexer metadata and subgraph data. To ensure redundancy of data and operations and enable systems to continue functioning despite individual component failures we want to account for the following areas as they relate to running PostgreSQL:",source:"@site/docs/launchpad/tutorials/postgresql_ha.md",sourceDirName:"launchpad/tutorials",slug:"/launchpad/tutorials/postgresql_ha",permalink:"/launchpad/tutorials/postgresql_ha",draft:!1,editUrl:"https://github.com/graphops/docs/edit/main/docs/launchpad/tutorials/postgresql_ha.md",tags:[],version:"current",frontMatter:{},sidebar:"launchpadSidebar",previous:{title:"Deploying a Monitoring stack with HA",permalink:"/launchpad/tutorials/monitoring-stack-with-HA"},next:{title:"Considerations for Kubernetes installation using FCOS",permalink:"/launchpad/advanced-tutorials/advanced-kubernetes"}},l={},p=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Configuring Postgresql with Zalando&#39;s Operator",id:"configuring-postgresql-with-zalandos-operator",level:2},{value:"Scalability",id:"scalability",level:2},{value:"Implementing WAL Archiving and Base Backups",id:"implementing-wal-archiving-and-base-backups",level:2},{value:"Setting up Clones and Standby Clusters",id:"setting-up-clones-and-standby-clusters",level:2},{value:"Promoting a standby cluster to a database cluster",id:"promoting-a-standby-cluster-to-a-database-cluster",level:3},{value:"Monitoring",id:"monitoring",level:2},{value:"Useful commands",id:"useful-commands",level:2}],c={toc:p},u="wrapper";function d(e){let{components:t,...a}=e;return(0,r.kt)(u,(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"overview-of-high-availability-in-postgresql"},"Overview of High Availability in PostgreSQL"),(0,r.kt)("p",null,"One of the prerequisites of running an indexer stack is currently using PostgreSQL as a database for storing indexer metadata and subgraph data. To ensure redundancy of data and operations and enable systems to continue functioning despite individual component failures we want to account for the following areas as they relate to running PostgreSQL:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Automatic Failover"),": the ability to automatically switch operations to standby replicas if the primary database server fails, ensuring service continuity.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Data Integrity and Consistency"),": the ability to maintain data integrity and consistency across primary and replica servers, even in failover scenarios, through WAL and Transaction Management")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Scalability"),": the ability to support scalability, allowing databases to handle increased loads by distributing read queries across replicas.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Disaster Recovery"),": planning for data backups and restores, PITRs(Point In Time Recoverys) ensuring that data is replicated to geographically diverse locations, protecting against site-wide failures.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Monitoring and Health Checks"),": continuous monitoring of database health and performance metrics to detect and address issues before they lead to downtime."))),(0,r.kt)("p",null,"This guide takes an indexer through the different steps needed to configure HA in PostgreSQL, utilising the ",(0,r.kt)("inlineCode",{parentName:"p"},"postgres-operator")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"graph")," namespaces as starting points."),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"A fully functional working Kubernetes cluster."),(0,r.kt)("li",{parentName:"ul"},"Object storage buckets for WAL (Write-Ahead Logs) archiving and base backups.")),(0,r.kt)("h2",{id:"configuring-postgresql-with-zalandos-operator"},"Configuring Postgresql with Zalando's Operator"),(0,r.kt)("p",null,"Launchpad leverages the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/zalando/postgres-operator"},"Zalando postgres-operator")," for seamless creation and management of ",(0,r.kt)("a",{parentName:"p",href:"https://www.postgresql.org/"},"PostgreSQL")," databases within Kubernetes, facilitating highly-available clusters with ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/zalando/patroni"},"Patroni"),"."),(0,r.kt)("p",null,"Following the deployment of the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/graphops/launchpad-namespaces/blob/main/postgres-operator/README.md"},(0,r.kt)("inlineCode",{parentName:"a"},"postgres-operator")," namespace"),", you're set to initiate PostgreSQL database creation."),(0,r.kt)("h2",{id:"scalability"},"Scalability"),(0,r.kt)("p",null,"The ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/graphops/launchpad-namespaces/blob/main/graph/README.md"},(0,r.kt)("inlineCode",{parentName:"a"},"graph")," namespace")," is preconfigured to deploy ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/graphops/launchpad-namespaces/blob/main/graph/values/_common/graph-database.yaml"},"one replica PostgreSQL")," for ",(0,r.kt)("inlineCode",{parentName:"p"},"subgraph-data")," and one for ",(0,r.kt)("inlineCode",{parentName:"p"},"indexer-metadata"),". To scale the number of replicas, simply modify the numberOfInstances attribute in your Helmfile. Example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"graph-database:\n    values:\n        resources:\n            postgres-cr-primary-subgraph-data:\n                spec:\n                    numberOfInstances: 3\n")),(0,r.kt)("p",null,"This configuration initiates a primary instance for handling writes and reads alongside two read-only replicas. The failover protocol, orchestrated by ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/zalando/patroni/blob/master/docs/kubernetes.rst"},"Patroni"),", promotes a replica to the primary role in the event of a primary node's failure."),(0,r.kt)("p",null,"Given that ",(0,r.kt)("inlineCode",{parentName:"p"},"graph-node")," query nodes require write permissions to run SQL migrations, the intended way to use read-only replicas with graph-node is the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'store:\n  primary:\n    enabled: true\n    connection: "postgresql://${PRIMARY_SUBGRAPH_DATA_PGUSER}:${PRIMARY_SUBGRAPH_DATA_PGPASSWORD}@${PRIMARY_SUBGRAPH_DATA_PGHOST}:${PRIMARY_SUBGRAPH_DATA_PGPORT}/${PRIMARY_SUBGRAPH_DATA_PGDATABASE}"\n    weight: 0\n  "primary.replicas.repl1":\n    enabled: true\n    connection: "postgresql://${PRIMARY_SUBGRAPH_DATA_PGUSER}:${PRIMARY_SUBGRAPH_DATA_PGPASSWORD}@${PRIMARY_SUBGRAPH_DATA_PGHOST_REPL}:${PRIMARY_SUBGRAPH_DATA_PGPORT}/${PRIMARY_SUBGRAPH_DATA_PGDATABASE}"\n    weight: 1\n')),(0,r.kt)("p",null,"The above ensures that write requests will be handled by the primary instance and read requests will be handled by replica instances."),(0,r.kt)("admonition",{type:"warning"},(0,r.kt)("p",{parentName:"admonition"},"Before setting up your ",(0,r.kt)("inlineCode",{parentName:"p"},"graph-node-query")," nodes with read-only replicas beware that there is a ongoing ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/graphprotocol/graph-node/issues/4326"},"issue")," that makes query results not deterministic because the read replicas can trail behind the main database, usually up to 30s.")),(0,r.kt)("p",null,"The data between servers is replicated through WAL(Write-Ahead Logs) streaming replication which allows for changes to be streamed in real-time or near real-time to replicas. By first recording changes in the Write-Ahead Logging (WAL) system instead of directly applying every change to the disk immediately, PostgreSQL prioritises data safety, enforces atomic writes and minimizes I/O operations. Another benefit of using WAL is that in case of system failure, the database can be rebuilt from the latest available base backup and by replaying the WAL files. Note that a base backup is a full copy of the database cluster's data files, taken at a specific point in time, which serves as a starting point for both recovery and replication processes."),(0,r.kt)("h2",{id:"implementing-wal-archiving-and-base-backups"},"Implementing WAL Archiving and Base Backups"),(0,r.kt)("p",null,"By archiving the WAL data we can support reverting to any time instant covered by the available WAL data: we simply install a prior base backup of the database, and replay the WAL just as far as the desired time. "),(0,r.kt)("p",null,"While ",(0,r.kt)("inlineCode",{parentName:"p"},"archive_mode")," is enabled by default, it requires specific configurations for functionality, including setting ",(0,r.kt)("inlineCode",{parentName:"p"},"AWS_ENDPOINT")," and providing valid bucket credentials. Verify ",(0,r.kt)("inlineCode",{parentName:"p"},"archive_mode")," using ",(0,r.kt)("inlineCode",{parentName:"p"},"patronictl edit-config")," within your database pod. To configure ",(0,r.kt)("inlineCode",{parentName:"p"},"AWS_ENDPOINT")," for archiving and backups, create a ",(0,r.kt)("inlineCode",{parentName:"p"},"postgres-env-config")," ConfigMap:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'kind: ConfigMap\napiVersion: v1\nmetadata:\n    name: postgres-env-config\n    namespace: postgres-operator # any namespace can be used\ndata:\n    AWS_ENDPOINT: http://your-object-storage-endpoint.com\n    AWS_S3_FORCE_PATH_STYLE: "true" # needed if your object storage solution uses path style bucket naming convention instead of DNS ie. Ceph\n    USE_WALG_BACKUP: "true"\n    USE_WALG_RESTORE: "true"\n    WALG_DISABLE_S3_SSE: "true"\n    BACKUP_NUM_TO_RETAIN: "4"\n    BACKUP_SCHEDULE: "00 02 * * sun"\n    WAL_BUCKET_SCOPE_PREFIX: ""\n    WAL_BUCKET_SCOPE_SUFFIX: ""\n')),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"When using object storage for WAL archiving by default the PostgreSQL Operator expects that the bucket endpoint follows the DNS style naming convention. If you\u2019re using object storage that follows path style naming convention for buckets (ie. Ceph) you need to pass ",(0,r.kt)("inlineCode",{parentName:"p"},"AWS_S3_FORCE_PATH_STYLE: \u201ctrue\u201d")," to the ",(0,r.kt)("inlineCode",{parentName:"p"},"postgres-operator")," configmap")),(0,r.kt)("p",null,"Ensure your databases are equipped with the necessary credentials for WAL files and ",(0,r.kt)("inlineCode",{parentName:"p"},"basebackups")," storage:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"graph-database:\n    values:\n        resources:\n            postgres-cr-primary-subgraph-data:\n              spec:\n                env:\n                  - name: WAL_S3_BUCKET\n                    value: <name-of-your-bucket>\n                  - name: AWS_ACCESS_KEY_ID\n                    valueFrom:\n                      secretKeyRef:\n                        name: subgraph-database-bucket-secret\n                        key: AWS_ACCESS_KEY_ID\n                  - name: AWS_SECRET_ACCESS_KEY\n                    valueFrom:\n                      secretKeyRef:\n                        name: subgraph-database-bucket-secret\n                        key: AWS_SECRET_ACCESS_KEY\n                  - name: BACKUP_NUM_TO_RETAIN\n                    value: \"2\"\n                  - name: BACKUP_SCHEDULE\n                    value: '00 00 * * sun'\n")),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"Important mentions about WAL files that can impact the availability of your cluster:"),(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Should a replica experience a failure, it's important to note that WAL files will be retained and not deleted until either the replica has been successfully recovered, or removed. This retention policy is crucial for ensuring data integrity and consistency across the database cluster. However, it can lead to rapid disk space consumption, posing a risk of exhausting available storage.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Whenever taking a basebackup (new replica, standby, etc), WAL files accumulate at a fast pace and exhaust disk space. Beware of that.")))),(0,r.kt)("h2",{id:"setting-up-clones-and-standby-clusters"},"Setting up Clones and Standby Clusters"),(0,r.kt)("p",null,"A Clone is a PIT copy of the production database that one would use for testing for development or for major upgrades - think of it as an independent staging area. The postgres-operator allows for two ways to create clones:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Clone from an S3 bucket (recommended)"),(0,r.kt)("li",{parentName:"ul"},"Clone directly from a live instance")),(0,r.kt)("p",null,"To clone from an S3 bucket you need to define a new PostgreSQL CRD resource with ",(0,r.kt)("inlineCode",{parentName:"p"},"spec.clone")," section defined. Example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: "acid.zalan.do/v1"\nkind: postgresql\nmetadata:\n  name: primary-subgraph-data-clone\nspec:\n  clone:\n    # can be found in the metadata.uid of the source cluster Postgresql resource def\n    uid: "efd12e58-5786-11e8-b5a7-06148230260c"\n    # name of the cluster being cloned\n    cluster: "primary-subgraph-data"\n    # the new cluster will be cloned using the latest backup available before the timestamp\n    timestamp: "2024-04-12T12:40:33+00:00"\n    # the below s3_ parameters are required only when using non AWS S3 object storage\n    s3_wal_path: "s3://<bucketname>/spilo/<source_db_cluster>/<UID>/wal"\n    s3_endpoint: <your-s3-endpoint>\n    s3_force_path_style: true\n  env:\n    - name: CLONE_AWS_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: primary-subgraph-data-bucket-secret\n          key: AWS_ACCESS_KEY_ID\n    - name: CLONE_AWS_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: primary-subgraph-data-bucket-secret\n          key: AWS_SECRET_ACCESS_KEY\n')),(0,r.kt)("p",null,"Cloning directly from your source DB cluster is done via ",(0,r.kt)("inlineCode",{parentName:"p"},"pg_basebackup"),". To use this feature simply define your clone's PostgreSQL CRD as above and leave out the ",(0,r.kt)("inlineCode",{parentName:"p"},"timestamp")," field from the clone section."),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"The operator will connect to the service of the source cluster by name. If the cluster is called test, then the connection string will look like host=test port=5432), which means that you can clone only from clusters within the same namespace."),(0,r.kt)("p",{parentName:"admonition"},"To set up a new standby or clone PostgreSQL instance that streams from a live instance, you must ensure that the new instance has the correct credentials. This involves ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/zalando/postgres-operator/blob/master/docs/user.md#providing-credentials-of-source-cluster"},"copying the credentials from the source cluster's secrets")," to successfully bootstrap the standby cluster or clone.")),(0,r.kt)("p",null,"A Standby Cluster is a cluster that first clones a database, and keeps replicating changes afterwards. It can exist in a different location than its source database, but unlike cloning, the PostgreSQL version between source and target cluster has to be the same. A Standby Cluster is a great way to ensure you have a disaster recovery plan if you main database fails."),(0,r.kt)("p",null,"Similarly to cloning you can start a Standby Cluster by streaming changes from archived WAL files or by streaming changes directly from your primary database. "),(0,r.kt)("p",null,"To start a cluster as a standby from archived WAL files, add the following ",(0,r.kt)("inlineCode",{parentName:"p"},"standby")," section in the Postgres CR definition:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: "acid.zalan.do/v1"\nkind: postgresql\nmetadata:\n  name: primary-subgraph-data-standby\nspec:\n  standby:\n    s3_wal_path: "s3://<bucketname>/spilo/<source_db_cluster>/<UID>/wal/<PGVERSION>"\n    s3_endpoint: <your-s3-endpoint>\n    s3_force_path_style: true # optional but needed if your object storage solution uses path style bucket naming convention instead of DNS ie. Ceph\n  env:\n    - name: STANDBY_AWS_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: primary-subgraph-data-bucket-secret\n          key: AWS_ACCESS_KEY_ID\n    - name: STANDBY_AWS_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: primary-subgraph-data-bucket-secret\n          key: AWS_SECRET_ACCESS_KEY\n')),(0,r.kt)("p",null,"To start a cluster as a standby from a remote primary, add the following ",(0,r.kt)("inlineCode",{parentName:"p"},"standby")," options in the PostgreSQL CRD definition, instead of the ",(0,r.kt)("inlineCode",{parentName:"p"},"s3_")," options:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  standby:\n    standby_host: "<your-source-db-host>.<namespace>"\n    standby_port: "5433"\n')),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"For standby clusters, specifying both S3 storage and remote live clusters as data sources is not possible; attempting to configure both simultaneously will result in an error. You must choose either S3 or a remote live cluster as the source but not both.")),(0,r.kt)("h3",{id:"promoting-a-standby-cluster-to-a-database-cluster"},"Promoting a standby cluster to a database cluster"),(0,r.kt)("p",null,"To promote a standby cluster to a proper database cluster you have to ensure it stops replicating changes from the source, and starts accepting writes instead. To promote, remove the standby section from the postgres cluster manifest. A rolling update will be triggered removing the STANDBY_* environment variables from the pods, followed by a Patroni config update that promotes the cluster."),(0,r.kt)("h2",{id:"monitoring"},"Monitoring"),(0,r.kt)("p",null,"By default the postgres-operator does not come setup up with any monitoring capabilities. However, to enable metric collection for database performance and WALs performance the following exporters can be used:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/prometheus-community/postgres_exporter"},"Prometheus exporter")," for PostgreSQL server metrics"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/thedatabaseme/wal-g-exporter"},"WAL-G exporter")," for gathering WAL-G backup metrics for Postgres databases")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"The above mentioned exporters are not the only ones that can be used. There is a large variety of PostgreSQL exporters that you can pick from, however the wal-g-exporter specifically has been used as it designed to work well with ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/zalando/spilo"},"Zalando's Spilo"),".")),(0,r.kt)("p",null,"To enable the use of the above exporters you need to pass the following configuration to the postgres-operator to run your postgresql dbs with exporter sidecars:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'postgres-operator:\n  values:\n    configGeneral:\n      sidecars:\n        - name: wal-g-exporter\n          image: ghcr.io/thedatabaseme/wal-g-exporter:0.3.1\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: HTTP_PORT\n            value: "9351"\n          - name: PGUSER\n            value: "$(POSTGRES_USER)"\n          - name: PGPASSWORD\n            value: "$(POSTGRES_PASSWORD)"\n          ports:\n          - name: wal-g-exporter\n            containerPort: 9351\n            protocol: TCP\n        - name: exporter\n          image: quay.io/prometheuscommunity/postgres-exporter:v0.15.0\n          ports:\n          - name: pg-exporter\n            containerPort: 9187\n            protocol: TCP\n          resources:\n            requests:\n              cpu: 50m\n              memory: 200M\n          env:\n          - name: CLUSTER_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.labels[\'cluster-name\']\n          - name: DATA_SOURCE_NAME\n            value: "postgresql://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@localhost:5432/postgres?sslmode=disable"\n')),(0,r.kt)("p",null,"Additionally the ",(0,r.kt)("inlineCode",{parentName:"p"},"wal-g-exporter")," relies on having all needed WAL-G environment variables within an envdir under ",(0,r.kt)("inlineCode",{parentName:"p"},"/run/etc/wal-e.d/env"),". As such you need to add the following spec to each postgresql CR definitions for the aforementioned path to be shared with the exporter sidecars:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: "acid.zalan.do/v1"\nkind: postgresql\nmetadata:\n  name: <you-db-name>\nspec:\n  additionalVolumes:\n    - name: walg\n      mountPath: /run/etc\n      targetContainers:\n        - postgres\n        - wal-g-exporter\n      volumeSource:\n        emptyDir: {}\n  # the rest of your spec goes here\n')),(0,r.kt)("p",null,"Once you've updated your ",(0,r.kt)("inlineCode",{parentName:"p"},"postgres-operator")," spec and the CR of each of your PostgreSQL databases it's time to add PodMonitors for Prometheus to track and collect the metrics. Example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: <your-database-name>\n  namespace: <your-database-namespace>\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/instance: <your-database-name>\n  podMetricsEndpoints:\n  - port: pg-exporter\n    path: /metrics\n    scrapeTimeout: "60s"\n    honorLabels: true\n  - port: wal-g-exporter\n    path: /metrics\n    scrapeTimeout: "60s"\n    honorLabels: true\n  - targetPort: 8008\n    path: /metrics\n    scrapeTimeout: "10s"\n    honorLabels: true\n  namespaceSelector:\n    matchNames:\n    - <your-database-namespace>\n')),(0,r.kt)("h2",{id:"useful-commands"},"Useful commands"),(0,r.kt)("p",null,"When using Zalando's operator, the pods running the PostgreSQL database come installed with useful scripts and CLIs as they are running the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/zalando/spilo"},"Spilo image"),". One such functionality is ",(0,r.kt)("inlineCode",{parentName:"p"},"patronictl")," which is used to manage and interact with your database cluster."),(0,r.kt)("p",null,"You can use the ",(0,r.kt)("inlineCode",{parentName:"p"},"patroni reinit <cluster_name> <member_name>")," in the event one of the cluster members needs to be reinitialized after falling out of sync with the primary due to corruption or other issues that prevent it from catching up through normal replication. It can also be used to clean a member that has been problematic or to refresh its data completely for consistency checks. This command resets a replica by wiping its existing data and then resynchronizing it from the current leader or another specified member of the cluster."))}d.isMDXComponent=!0}}]);