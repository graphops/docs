"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2625],{3905:(e,n,a)=>{a.d(n,{Zo:()=>u,kt:()=>h});var t=a(7294);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function s(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?s(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function i(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},s=Object.keys(e);for(t=0;t<s.length;t++)a=s[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(t=0;t<s.length;t++)a=s[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=t.createContext({}),c=function(e){var n=t.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},u=function(e){var n=c(e.components);return t.createElement(l.Provider,{value:n},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},m=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,s=e.originalType,l=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),p=c(a),m=r,h=p["".concat(l,".").concat(m)]||p[m]||d[m]||s;return a?t.createElement(h,o(o({ref:n},u),{},{components:a})):t.createElement(h,o({ref:n},u))}));function h(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var s=a.length,o=new Array(s);o[0]=m;var i={};for(var l in n)hasOwnProperty.call(n,l)&&(i[l]=n[l]);i.originalType=e,i[p]="string"==typeof e?e:r,o[1]=i;for(var c=2;c<s;c++)o[c]=a[c];return t.createElement.apply(null,o)}return t.createElement.apply(null,a)}m.displayName="MDXCreateElement"},2884:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var t=a(7462),r=(a(7294),a(3905));const s={},o="Deploying a Monitoring stack with HA",i={unversionedId:"launchpad/guides/monitoring-stack-with-HA",id:"launchpad/guides/monitoring-stack-with-HA",title:"Deploying a Monitoring stack with HA",description:"Prerequisites",source:"@site/docs/launchpad/guides/monitoring-stack-with-HA.md",sourceDirName:"launchpad/guides",slug:"/launchpad/guides/monitoring-stack-with-HA",permalink:"/launchpad/guides/monitoring-stack-with-HA",draft:!1,editUrl:"https://github.com/graphops/docs/edit/main/docs/launchpad/guides/monitoring-stack-with-HA.md",tags:[],version:"current",frontMatter:{},sidebar:"launchpadSidebar",previous:{title:"Kubernetes Guide - Bootstrapping with Kubeadm",permalink:"/launchpad/guides/kubernetes-create-cluster-with-kubeadm"},next:{title:"Overview of High Availability in PostgreSQL",permalink:"/launchpad/guides/postgresql_ha"}},l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Configuring Loki for HA",id:"configuring-loki-for-ha",level:2},{value:"Setting up a Prometheus Stack with HA",id:"setting-up-a-prometheus-stack-with-ha",level:2},{value:"Thanos",id:"thanos",level:3},{value:"Prometheus Stack",id:"prometheus-stack",level:3}],u={toc:c},p="wrapper";function d(e){let{components:n,...a}=e;return(0,r.kt)(p,(0,t.Z)({},u,a,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"deploying-a-monitoring-stack-with-ha"},"Deploying a Monitoring stack with HA"),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"A fully functional working Kubernetes cluster"),(0,r.kt)("li",{parentName:"ul"},"Two object storage buckets: one for Logs data, used by Loki, and one for Metrics data, used by Thanos")),(0,r.kt)("h2",{id:"configuring-loki-for-ha"},"Configuring Loki for HA"),(0,r.kt)("p",null,"Launchpad uses the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/grafana/helm-charts/tree/main/charts/loki-distributed"},"loki-distributed")," release for setting up Loki, which can be configured according to its values interface (as seen ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/grafana/helm-charts/blob/main/charts/loki-distributed/values.yaml"},"here"),")"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Note"),": The example setups we'll show will be based on an architecture that makes use of the following components: querier, distributor, ingester, queryFrontend, gateway, compactor, ruler, indexGateway. Different architectures are possible so adjust to your needs as necessary."),(0,r.kt)("p",null,"For an HA setup, deploying several components with multiple replicas each, loki-distributed values can be set like in the following example snippet:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"querier:\n  replicas: 2\n  maxUnavailable: 1\ndistributor:\n  replicas: 3\n  maxUnavailable: 2\ningester:\n  replicas: 3\n  maxUnavailable: 2\nqueryFrontend:\n  replicas: 2\n  maxUnavailable: 1\ngateway:\n  replicas: 2\n  maxUnavailable: 1\ncompactor:\n  kind: Deployment\n  replicas: 1\n  enabled: true\nruler:\n  enabled: true\n  replicas: 2\n  maxUnavailable: 1\nindexGateway:\n  enabled: true\n  replicas: 2\n  maxUnavailable: 1\nloki:\n  structuredConfig:\n    ruler:\n      ring:\n        kvstore:\n          store: memberlist\n    ingester:\n      lifecycler:\n        ring:\n          replication_factor: 2\n")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Note"),": If you use a compactor, only one will run at a time and it's not critical so you don't really need more than one instance of it."),(0,r.kt)("p",null,"Besides increasing the number of replicas, ingester ",(0,r.kt)("inlineCode",{parentName:"p"},"replication_factor")," is of particular relevance as the Distributor will distribute the write load to multiple ingesters and will require a quorum of them to have acknowledged the write (replication_factor / 2 + 1). For lowering the chances of loosing logs, a replication_factor of at least two should be used (Loki default is 3)."),(0,r.kt)("p",null,"Loki's storage fundamentally requires object storage, regardless of whether HA is used or if there's more than one replica for any component, as multiple components need to share this storage."),(0,r.kt)("p",null,"Object storage can be setup as shown in the following snippet:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'loki:\n  structuredConfig:\n    storage_config:\n      tsdb_shipper:\n        active_index_directory: /var/loki/data/tsdb-index\n        cache_location: /var/loki/data/tsdb-cache\n        index_gateway_client:\n          # only applicable if using microservices where index-gateways are independently deployed.\n          # This example is using kubernetes-style naming.\n          server_address: dns:///loki-loki-distributed-index-gateway.monitoring.svc.cluster.local:9095\n        shared_store: s3\n      aws:\n        bucketnames: <<bucket>>\n        endpoint: <<endpoint>>\n        region: <<region>>\n        access_key_id: "${S3_ACCESS_KEY_ID}"\n        secret_access_key: "${S3_SECRET_ACCESS_KEY}"\n        insecure: false\n        sse_encryption: false\n        s3forcepathstyle: true\n    schema_config:\n      configs:\n        # New TSDB schema below\n        - from: "2024-01-01"\n          index:\n            period: 24h\n            prefix: index_\n          object_store: s3\n          schema: v12\n          store: tsdb\n    query_scheduler:\n      # the TSDB index dispatches many more, but each individually smaller, requests.\n      # We increase the pending request queue sizes to compensate.\n      max_outstanding_requests_per_tenant: 32768\n    querier:\n      max_concurrent: 16\n    compactor:\n      shared_store: s3\nquerier:\n  extraArgs:\n    - -config.expand-env=true\n  extraEnv:\n    - name: S3_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_ACCESS_KEY_ID\n    - name: S3_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_SECRET_ACCESS_KEY\ndistributor:\n  extraArgs:\n    - -config.expand-env=true\n  extraEnv:\n    - name: S3_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_ACCESS_KEY_ID\n    - name: S3_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_SECRET_ACCESS_KEY\ningester:\n  extraArgs:\n    - -config.expand-env=true\n  extraEnv:\n    - name: S3_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_ACCESS_KEY_ID\n    - name: S3_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_SECRET_ACCESS_KEY\ncompactor:\n  extraArgs:\n    - -config.expand-env=true\n  extraEnv:\n    - name: S3_ACCESS_KEY_ID\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_ACCESS_KEY_ID\n    - name: S3_SECRET_ACCESS_KEY\n      valueFrom:\n        secretKeyRef:\n          name: <<bucket-secret>>\n          key: S3_SECRET_ACCESS_KEY\n')),(0,r.kt)("p",null,"Here we are setting up object storage in the ",(0,r.kt)("inlineCode",{parentName:"p"},"structuredConfig")," section, but to keep the credentials secret, we are adding env vars from secrets to several components and an extra command line argument ",(0,r.kt)("inlineCode",{parentName:"p"},"-config.expand-env=true"),", the purpose of which is being able to use ENV vars in the ",(0,r.kt)("inlineCode",{parentName:"p"},"structuredConfig")," section. With that argument, the components will replace the values such as ",(0,r.kt)("inlineCode",{parentName:"p"},"${S3_ACCESS_KEY_ID}")," by the corresponding ENV var value upon processing the config."),(0,r.kt)("p",null,"Besides setting up object storage, we're also configuring TSDB index schema in substitution of the default boltdb-shipper, which is a more recent and more efficient alternative to it. Doing so is not mandatory but recommended."),(0,r.kt)("p",null,"Putting it all together and adding a few more standard options such as persistence (PVC) to some components, and enabling ServiceMonitor and Prometheus Rules, a Launchpad Monitoring namespace ",(0,r.kt)("inlineCode",{parentName:"p"},"helmfile.yaml")," Loki config could look like:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'helmfiles:\n  - path: git::https://github.com/graphops/launchpad-namespaces.git@monitoring/helmfile.yaml?ref=monitoring-stable/latest\n    selectorsInherited: true\n    values:\n    - features: [ metrics, logs]\n      loki:\n        values:\n          loki:\n            structuredConfig:\n              ingester:\n                # Disable chunk transfer which is not possible with statefulsets\n                # and unnecessary for boltdb-shipper\n                max_transfer_retries: 0\n                chunk_idle_period: 1h\n                chunk_target_size: 1536000\n                max_chunk_age: 1h\n              storage_config:\n                tsdb_shipper:\n                  active_index_directory: /var/loki/data/tsdb-index\n                  cache_location: /var/loki/data/tsdb-cache\n                  index_gateway_client:\n                    # only applicable if using microservices where index-gateways are independently deployed.\n                    # This example is using kubernetes-style naming.\n                    server_address: dns:///loki-loki-distributed-index-gateway.monitoring.svc.cluster.local:9095\n                  shared_store: s3\n                aws:\n                  bucketnames: <<bucket>>\n                  endpoint: <<endpoint>>\n                  region: <<region>>\n                  access_key_id: "${S3_ACCESS_KEY_ID}"\n                  secret_access_key: "${S3_SECRET_ACCESS_KEY}"\n                  insecure: false\n                  sse_encryption: false\n                  s3forcepathstyle: true\n              schema_config:\n                configs:\n                  # New TSDB schema below\n                  - from: "2024-01-01"\n                    index:\n                      period: 24h\n                      prefix: index_\n                    object_store: s3\n                    schema: v12\n                    store: tsdb\n              query_scheduler:\n                # the TSDB index dispatches many more, but each individually smaller, requests.\n                # We increase the pending request queue sizes to compensate.\n                max_outstanding_requests_per_tenant: 32768\n              querier:\n                # Each `querier` component process runs a number of parallel workers to process queries simultaneously.\n                # You may want to adjust this up or down depending on your resource usage\n                # (more available cpu and memory can tolerate higher values and vice versa),\n                # but we find the most success running at around `16` with tsdb\n                max_concurrent: 16\n              compactor:\n                shared_store: s3\n              ruler:\n                ring:\n                  kvstore:\n                    store: memberlist\n                rule_path: /tmp/loki/scratch\n                alertmanager_url: http://kube-prometheus-stack-alertmanager:9093\n                external_url: <<your alertmanager external URL>>\n\n          querier:\n            replicas: 2\n            maxUnavailable: 1\n            extraArgs:\n              - -config.expand-env=true\n            extraEnv:\n              - name: S3_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_ACCESS_KEY_ID\n              - name: S3_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_SECRET_ACCESS_KEY\n          distributor:\n            replicas: 3\n            maxUnavailable: 2\n            extraArgs:\n              - -config.expand-env=true\n            extraEnv:\n              - name: S3_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_ACCESS_KEY_ID\n              - name: S3_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_SECRET_ACCESS_KEY\n          ingester:\n            replicas: 3\n            maxUnavailable: 2\n            persistence:\n              enabled: true\n              inMemory: false\n              claims:\n                - name: data\n                  size: 10Gi\n            extraArgs:\n              - -config.expand-env=true\n            extraEnv:\n              - name: S3_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_ACCESS_KEY_ID\n              - name: S3_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_SECRET_ACCESS_KEY\n          queryFrontend:\n            replicas: 2\n            maxUnavailable: 1\n          gateway:\n            replicas: 2\n            maxUnavailable: 1\n          compactor:\n            kind: Deployment\n            replicas: 1\n            enabled: true\n            extraArgs:\n              - -config.expand-env=true\n            extraEnv:\n              - name: S3_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_ACCESS_KEY_ID\n              - name: S3_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name: <<bucket-secret>>\n                    key: S3_SECRET_ACCESS_KEY\n          ruler:\n            enabled: true\n            replicas: 2\n            maxUnavailable: 1\n          indexGateway:\n            enabled: true\n            replicas: 2\n            maxUnavailable: 1\n          serviceMonitor:\n            enabled: true\n          prometheusRule:\n            enabled: true\n            namespace: monitoring\n')),(0,r.kt)("p",null,"We've also added setting up Ruler with Alertmanager's endpoint (which can be deployed by the Monitoring Namespace as well, as will be seen in the Metrics section)."),(0,r.kt)("h2",{id:"setting-up-a-prometheus-stack-with-ha"},"Setting up a Prometheus Stack with HA"),(0,r.kt)("h3",{id:"thanos"},"Thanos"),(0,r.kt)("p",null,"For an HA Prometheus Stack we'll need ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/thanos-io/thanos"},"Thanos")," which is not yet part of the Monitoring Namespace, so we'll start by going over how to deploy it with Launchpad."),(0,r.kt)("p",null,"Thanos requires object storage so a bucket (and credentials) will be needed. To deploy Thanos we're going to use bitnami's ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/bitnami/charts/tree/main/bitnami/thanos"},"thanos chart"),", and we'll deploy that with Launchpad as in the following example helmfile:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'repositories:\n- name: bitnami\n  url: https://charts.bitnami.com/bitnami\n          \nreleases:\n  - name: thanos\n    namespace: monitoring\n    createNamespace: true\n    chart: bitnami/thanos\n    version: ~12.20\n    missingFileHandler: Warn\n    values:\n    - existingObjstoreSecret: <<thanos-objstore-secret>>\n      query:\n        replicaCount: 2\n        dnsDiscovery:\n          sidecarsService: "prometheus-operated"\n          sidecarsNamespace: "monitoring"\n        replicaLabel:\n          - prometheus_replica\n      queryFrontend:\n        enabled: true\n        replicaCount: 2\n      compactor:\n        enabled: true\n        persistence:\n          enabled: true\n        retentionResolutionRaw: 30d\n        retentionResolution5m: 30d\n        retentionResolution1h: 10y\n      storegateway:\n        enabled: true\n        replicaCount: 2\n        persistence:\n          enabled: true\n      metrics:\n        enabled: true\n        serviceMonitor:\n          enabled: true\n      prometheusRule:\n        enabled: true\n')),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Warning"),": Never try to run more than one instance of compactor. If your object storage does not support locking, it will lead to error states."),(0,r.kt)("p",null,"Where we added the bitnami repository, and a release to deploy the Thanos chart from that repository. From the values used in this example, notice the ",(0,r.kt)("inlineCode",{parentName:"p"},"query.dnsDiscovery")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"query.replicaLabel")," keys, as those values need to match the ones used in Thanos Prometheus sidecar, deployed in the ",(0,r.kt)("inlineCode",{parentName:"p"},"kube-prometheus-stack")," release with the Monitoring Namespace."),(0,r.kt)("p",null,"There is one extra thing needed for Thanos, a secret with the bucket credentials as referred previously with ",(0,r.kt)("inlineCode",{parentName:"p"},"<<thanos-objstore-secret>>"),". That secret need to have a key called ",(0,r.kt)("inlineCode",{parentName:"p"},"objstore.yml"),", and its value content should be yaml and have keys like these:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"type: S3\nconfig: \n  endpoint: <<endpoint>>\n  bucket: <<bucket name>>\n  bucket_lookup_type: path \n  insecure: false\n  access_key: <<access_key>>\n  secret_key: <<secret_key>>\n")),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"bucket_lookup_type")," can be ",(0,r.kt)("inlineCode",{parentName:"p"},"auto"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"path")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"virtual_host"),", and you would want to use ",(0,r.kt)("inlineCode",{parentName:"p"},"path")," for Ceph Object Storage. You can check ",(0,r.kt)("a",{parentName:"p",href:"https://thanos.io/tip/thanos/storage.md/#s3"},"here")," all the available options."),(0,r.kt)("p",null,"Adding a secret like that with Launchpad and Sealed Secrets will add a release like so:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"releases:\n  - name: thanos-objstore-secret\n    namespace: monitoring\n    chart: graphops/resource-injector\n    values:\n      - resources:\n          thanos-objstore-secret:\n            apiVersion: bitnami.com/v1alpha1\n            kind: SealedSecret\n            metadata:\n              name: thanos-objstore-secret\n              namespace: monitoring\n            spec:\n              encryptedData:\n                objstore.yml: <<SealedSecrets Encrypted Data>>\n")),(0,r.kt)("p",null,"The last remaining required Thanos component, the prometheus sidecar, will be deployed with ",(0,r.kt)("inlineCode",{parentName:"p"},"kube-prometheus-stack")," so keep reading."),(0,r.kt)("h3",{id:"prometheus-stack"},"Prometheus Stack"),(0,r.kt)("p",null,"There are three components we want to focus on, Prometheus, Grafana and Alertmanager.\nWe'll start with adjusting Alertmanager's config for HA which is the simplest:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"alertmanager:\n  alertmanagerSpec:\n    replicas: 3\n")),(0,r.kt)("p",null,"This will change our setup to a 3 replica Alertmanager and that's all that's required."),(0,r.kt)("p",null,"For grafana we have the added requirement of changing from the default embedded sqlite database to a shared database like Postgres."),(0,r.kt)("p",null,"So let's start by adding a release to create a Postgres database with the Postgres-Operator (from Launchpad Postgres-Operator Namespace). As an example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'releases:\n  - name: grafana-database\n    namespace: monitoring\n    createNamespace: true\n    chart: graphops/resource-injector\n    version: 0.2.0\n    missingFileHandler: Warn\n    values:\n      - resources:\n          grafana-database:\n            apiVersion: "acid.zalan.do/v1"\n            kind: postgresql\n            metadata:\n              name: grafana-database\n              teamId: "pg"\n              numberOfInstances: 2\n              users:\n                grafana:\n                  - superuser\n                  - createdb\n              enableMasterLoadBalancer: false\n              enableReplicaLoadBalancer: false\n              enableConnectionPooler: false\n              enableReplicaConnectionPooler: false\n              databases:\n                grafana: grafana\n              postgresql:\n                version: "15"\n                parameters: {}\n              volume:\n                size: 1Gi\n                storageClass: <<your_storage_class>>\n              resources:\n                requests:\n                  cpu: 250m\n                  memory: 1Gi\n                limits:\n                  cpu: 1000m\n                  memory: 4Gi\n              patroni:\n                initdb:\n                  encoding: "UTF8"\n                  locale: "C"\n                pg_hba:\n                  - local   all             all                                   trust\n                  - hostssl all             +zalandos    127.0.0.1/32       pam\n                  - host    all             all                127.0.0.1/32       md5\n                  - hostssl all             +zalandos    ::1/128            pam\n                  - host    all             all                ::1/128            md5\n                  - local   replication     standby                    trust\n                  - hostssl replication     standby all                md5\n                  - hostnossl all           all                all                md5\n                  - hostssl all             +zalandos    all                pam\n                  - hostssl all             all                all                md5\n              podAnnotations:\n                coa.zalan.do/auto-create-database: "true"\n')),(0,r.kt)("p",null,"Having that database, adjusting grafana values setup can be achieved like so:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'grafana:\n  replicas: 2\n  envValueFrom:\n    DATABASE_PASSWORD:\n      secretKeyRef:\n        name: grafana.grafana-database.credentials.postgresql.acid.zalan.do\n        key: password\n  sidecar:\n    datasources:\n      url: http://thanos-query-frontend:9090\n      createPrometheusReplicasDatasources: false\n  grafana.ini:\n    database:\n      type: postgres\n      host: grafana-database.monitoring.svc:5432\n      name: grafana\n      user: grafana\n      password: "$__env{DATABASE_PASSWORD}"\n')),(0,r.kt)("p",null,"Finally, we need to adjust Prometheus to increase replicas and use Thanos sidecar."),(0,r.kt)("p",null,"A workable set of values for accomplishing that looks like:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"prometheus:\n  prometheusSpec:\n    replicas: 2\n    shards: 1\n    thanos:\n      objectStorageConfig:\n        existingSecret:\n          name: <<thanos-objstore-secret>>\n          key: objstore.yml\n    replicaExternalLabelName: prometheus_replica\n  thanosService:\n    enabled: true\n  thanosServiceMonitor:\n    enabled: true\n")),(0,r.kt)("p",null,"Notice the sidecar will be configured to use the same secret provisioned before for Thanos, and the ",(0,r.kt)("inlineCode",{parentName:"p"},"replicaExternalLabelName")," matches the value used before as well."),(0,r.kt)("p",null,"Taking all of this together, here's an example of an helmfile that deploys Thanos and ",(0,r.kt)("inlineCode",{parentName:"p"},"kube-prometheus-stack")," setting the most important values for HA:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'repositories:\n- name: bitnami\n  url: https://charts.bitnami.com/bitnami\n\nhelmfiles:\n  - path: git::https://github.com/graphops/launchpad-namespaces.git@monitoring/helmfile.yaml?ref=monitoring-stable/latest\n    selectorsInherited: true\n    values:\n    - helmDefaults:\n        <<: *helmDefaults\n      features: [ metrics, logs]\n      kube-prometheus-stack:\n        values:\n          kube-prometheus-stack:\n            alertmanager:\n              alertmanagerSpec:\n                replicas: 3\n            grafana:\n              replicas: 2\n              envValueFrom:\n                DATABASE_PASSWORD:\n                  secretKeyRef:\n                    name: grafana.grafana-database.credentials.postgresql.acid.zalan.do\n                    key: password\n              sidecar:\n                datasources:\n                  url: http://thanos-query-frontend:9090\n                  createPrometheusReplicasDatasources: false\n              grafana.ini:\n                database:\n                  type: postgres\n                  host: grafana-database.monitoring.svc:5432\n                  name: grafana\n                  user: grafana\n                  password: "$__env{DATABASE_PASSWORD}"\n            prometheus:\n              prometheusSpec:\n                replicas: 2\n                shards: 1\n                thanos:\n                  objectStorageConfig:\n                    existingSecret:\n                      name: <<thanos-objstore-secret>>\n                      key: objstore.yml\n                replicaExternalLabelName: prometheus_replica\n              thanosService:\n                enabled: true\n              thanosServiceMonitor:\n                enabled: true\n\nreleases:\n  - name: thanos-objstore-secret\n    namespace: monitoring\n    chart: graphops/resource-injector\n    values:\n      - resources:\n          thanos-objstore-secret:\n            apiVersion: bitnami.com/v1alpha1\n            kind: SealedSecret\n            metadata:\n              name: thanos-objstore-secret\n              namespace: monitoring\n            spec:\n              encryptedData:\n                objstore.yml: <<SealedSecrets Encrypted Data>>\n\n  - name: thanos\n    namespace: monitoring\n    createNamespace: true\n    chart: bitnami/thanos\n    version: ~12.20\n    missingFileHandler: Warn\n    values:\n    - existingObjstoreSecret: <<thanos-objstore-secret>>\n      query:\n        replicaCount: 2\n        dnsDiscovery:\n          sidecarsService: "prometheus-operated"\n          sidecarsNamespace: "monitoring"\n        replicaLabel:\n          - prometheus_replica\n      queryFrontend:\n        enabled: true\n        replicaCount: 2\n      compactor:\n        enabled: true\n        persistence:\n          enabled: true\n        retentionResolutionRaw: 30d\n        retentionResolution5m: 30d\n        retentionResolution1h: 10y\n      storegateway:\n        enabled: true\n        replicaCount: 2\n        persistence:\n          enabled: true\n      metrics:\n        enabled: true\n        serviceMonitor:\n          enabled: true\n      prometheusRule:\n        enabled: true\n\n  - name: grafana-database\n    namespace: monitoring\n    createNamespace: true\n    chart: graphops/resource-injector\n    version: 0.2.0\n    missingFileHandler: Warn\n    values:\n      - resources:\n          grafana-database:\n            apiVersion: "acid.zalan.do/v1"\n            kind: postgresql\n            metadata:\n              name: grafana-database\n              teamId: "pg"\n              numberOfInstances: 2\n              users:\n                grafana:\n                  - superuser\n                  - createdb\n              enableMasterLoadBalancer: false\n              enableReplicaLoadBalancer: false\n              enableConnectionPooler: false\n              enableReplicaConnectionPooler: false\n              databases:\n                grafana: grafana\n              postgresql:\n                version: "15"\n                parameters: {}\n              volume:\n                size: 1Gi\n                storageClass: <<your_storage_class>>\n              resources:\n                requests:\n                  cpu: 250m\n                  memory: 1Gi\n                limits:\n                  cpu: 1000m\n                  memory: 4Gi\n              patroni:\n                initdb:\n                  encoding: "UTF8"\n                  locale: "C"\n                pg_hba:\n                  - local   all             all                                   trust\n                  - hostssl all             +zalandos    127.0.0.1/32       pam\n                  - host    all             all                127.0.0.1/32       md5\n                  - hostssl all             +zalandos    ::1/128            pam\n                  - host    all             all                ::1/128            md5\n                  - local   replication     standby                    trust\n                  - hostssl replication     standby all                md5\n                  - hostnossl all           all                all                md5\n                  - hostssl all             +zalandos    all                pam\n                  - hostssl all             all                all                md5\n              podAnnotations:\n                coa.zalan.do/auto-create-database: "true"\n\n')))}d.isMDXComponent=!0}}]);